---
title: "Introduction to Bayesian Modeling"
author: "Keith McNulty"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float:
      toc_collapsed: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
```

## Bayesian Updating

Imagine that you are invited to your neighbours fancy dress party, but you don't know what the theme is.  You have access to a few different costumes, and you need to decide which one to choose.  You could take two approaches to making this decision:

1.  Based on your knowledge of the hosts, and because you want to get in there as soon as possible, you can make an assumption about the theme, get into an appropriate costume and enter the party.  Because it's a party, you'll be able to interact with people and after a while you'll work out how well you chose your costume.  But if you want to change it you'll have to admit defeat, head back to your place and get changed and start again.

2.  You can wait and watch other people enter the party.  Maybe they are not all wearing exactly the same theme, but as you watch each person go in, you can start to update your initial assumptions until you get to a point where you feel you are certain enough to choose a costume without fear of embarassment for getting the theme wrong.  This will take longer, but there's less up front risk of getting it wrong.

In classical statistical models like linear or logistic regression, we are keen to get into the party asap. We make an ingoing assumption about the distribution of our outcome in our population.  For example, we assume a binomial or a Poisson distribution.  In most classical models, this assumption is paramount and remains unviolated -- we do not revisit it throughout the modeling process.  It is a 'truth' from the moment we make the assumption.  Of course, if this assumption is bad, it will lead to useless inferences, and that is a common reason why statistical models can be ineffective or misleading.  We might have to leave the party and choose a different costume.

In Bayesian modeling, we make no such unviolated assumption.  Instead we stake out the neighbour's place for a while.  We construct our model from our data observation by observation.  At the beginning, we make the best assumption we can about the likelihood of our outcome.  In many cases we can make no assumption at all about the likelihood of our outcome and therefore we are *indifferent*, meaning we allow equal likelihood of any outcome possibility.   After each observation we process, we *update* the likelihood of our outcome on the basis of the new information.  This process is known as *Bayesian Updating*.  We call the likelihood function of our outcome before the observation our *prior*, and after the observation we call it our *posterior*.  

Given that we learn about the likelihood of our outcome observation by observation, this is an optimal way of modeling our sample, but it is often computationally expensive compared to classical models.  In fact, this was one of the main reasons why Bayesian analysis was disregarded in the early- to mid- 20th century -- it was not considered practical.  Amazing progress in computational science has now made Bayesian analysis much more approachable.

But Bayesian analysis is not perfect and will not always create a great model.  Poor assumptions about priors can get you off to a terrible start, and sampling bias is just as much a risk as in classical statistical methods.  

## Global Water example

In Richard McElreath's book *Statistical Rethinking*, this example is used to illustrate a Bayesian Updating process.  Imagine we want to determine the proportion $p$ of water on the Earth's surface area.  Assume we initially have no idea about this, so we assume that $p$ has equal likelihood of taking any value between 0 and 1.  

Now imagine that we sample using the following experiment - we spin a globe and then stop it at any point with our finger and record whether the place we stopped it at was water or land.  Now, since there are only two values for this sampling, we know that the probabilty of observing $W$ waters and $L$ lands after $W+L$ observations obeys the binomial distribution as follows:

$$
P(W, L \mid p) = \frac{(W+L)!}{W!L!}p^{W}(1-p)^{L}
$$
Now let's take the first observation.  It's $W$.  So we can calculate that $P(1, 0 \mid p) = p$.  This give us the following posterior distributuion for $p$ after this observation.

```{r}
library(ggplot2)
library(latex2exp)

base_plot <- ggplot() +
  xlim(0, 1) +
  ylim(0, 1) +
  xlab(TeX(r'($p$)')) +
  ylab("Likelihood") + 
  theme_minimal()

p1 <- base_plot +
  geom_function(fun = function (p) p, color = "blue") + 
  labs(title = TeX(r'($W$)'))

p1

```

Now let's take a second observation.  It's $L$.  We now have $P(1, 1 \mid p) = 2p(1-p)$.  Let's plot our posterior and prior for our second observation.

```{r}
p2 <- base_plot +
  geom_function(fun = function (p) p, linetype = "dashed", color = "red") +
  geom_function(fun = function (p) {2*p*(1-p)},  color = "blue") +
  labs(title = TeX(r'($WL$)'))

p2

```

For a third observation of $W$, we have $P(2, 1 \mid p) = 3p^2(1-p)$, so our prior and posterior will be as follows:

```{r}
p3 <- base_plot +
  geom_function(fun = function (p) {2*p*(1-p)}, linetype = "dashed", color = "red") +
  geom_function(fun = function (p) {3*p^2*(1-p)},  color = "blue") +
  labs(title = TeX(r'($WLW$)'))

p3
```

We can iterate this for, say, 16 observations, making use of the `dbinom()` function to plot the appropriate binomial functions in each case:

```{r}
base_plot <- ggplot() +
  xlim(0, 1) +
  ylim(0, 1) +
  xlab("p") +
  theme_minimal() +
  theme(axis.text = element_text(size = 5),
        axis.title.x = element_text(size = 6),
        axis.title.y = element_blank(),
        title = element_text(size = 6))

p1 <- base_plot +
  geom_function(fun = function (p) dbinom(1, size = 1, prob = p),  color = "blue") +
  labs(title = TeX(r'($W$)'))

p2 <- base_plot +
  geom_function(fun = function (p) dbinom(1, size = 1, prob = p), linetype = "dashed", color = "red") +
  geom_function(fun = function (p) dbinom(1, size = 2, prob = p),  color = "blue") +
  labs(title = TeX(r'($WL$)'))

p3 <- base_plot +
  geom_function(fun = function (p) dbinom(1, size = 2, prob = p), linetype = "dashed", color = "red") +
  geom_function(fun = function (p) dbinom(2, size = 3, prob = p),  color = "blue") +
  labs(title = TeX(r'($WLW$)'))

p4 <- base_plot +
  geom_function(fun = function (p) dbinom(2, size = 3, prob = p), linetype = "dashed", color = "red") +
  geom_function(fun = function (p) dbinom(3, size = 4, prob = p),  color = "blue") +
  labs(title = TeX(r'($WLWW$)'))

p5 <- base_plot +
  geom_function(fun = function (p) dbinom(3, size = 4, prob = p), linetype = "dashed", color = "red") +
  geom_function(fun = function (p) dbinom(4, size = 5, prob = p),  color = "blue") +
  labs(title = TeX(r'($WLWWW$)'))

p6 <- base_plot +
  geom_function(fun = function (p) dbinom(4, size = 5, prob = p), linetype = "dashed", color = "red") +
  geom_function(fun = function (p) dbinom(4, size = 6, prob = p),  color = "blue") +
  labs(title = TeX(r'($WLWWWL$)'))

p7 <- base_plot +
  geom_function(fun = function (p) dbinom(4, size = 6, prob = p), linetype = "dashed", color = "red") +
  geom_function(fun = function (p) dbinom(5, size = 7, prob = p),  color = "blue") +
  labs(title = TeX(r'($WLWWWLW$)'))

p8 <- base_plot +
  geom_function(fun = function (p) dbinom(5, size = 7, prob = p), linetype = "dashed", color = "red") +
  geom_function(fun = function (p) dbinom(5, size = 8, prob = p),  color = "blue") +
  labs(title = TeX(r'($WLWWWLWL$)'))

p9 <- base_plot +
  geom_function(fun = function (p) dbinom(5, size = 8, prob = p), linetype = "dashed", color = "red") +
  geom_function(fun = function (p) dbinom(6, size = 9, prob = p),  color = "blue") +
  labs(title = TeX(r'($WLWWWLWLW$)'))

p10 <- base_plot +
  geom_function(fun = function (p) dbinom(6, size = 9, prob = p), linetype = "dashed", color = "red") +
  geom_function(fun = function (p) dbinom(7, size = 10, prob = p),  color = "blue") +
  labs(title = TeX(r'($WLWWWLWLWW$)'))

p11 <- base_plot +
  geom_function(fun = function (p) dbinom(7, size = 10, prob = p), linetype = "dashed", color = "red") +
  geom_function(fun = function (p) dbinom(8, size = 11, prob = p),  color = "blue") +
  labs(title = TeX(r'($WLWWWLWLWWW$)'))

p12 <- base_plot +
  geom_function(fun = function (p) dbinom(8, size = 11, prob = p), linetype = "dashed", color = "red") +
  geom_function(fun = function (p) dbinom(9, size = 12, prob = p),  color = "blue") +
  labs(title = TeX(r'($WLWWWLWLWWWW$)'))

p13 <- base_plot +
  geom_function(fun = function (p) dbinom(9, size = 12, prob = p), linetype = "dashed", color = "red") +
  geom_function(fun = function (p) dbinom(9, size = 13, prob = p),  color = "blue") +
  labs(title = TeX(r'($WLWWWLWLWWWWL$)'))

p14 <- base_plot +
  geom_function(fun = function (p) dbinom(9, size = 13, prob = p), linetype = "dashed", color = "red") +
  geom_function(fun = function (p) dbinom(10, size = 14, prob = p),  color = "blue") +
  labs(title = TeX(r'($WLWWWLWLWWWWLW$)'))

p15 <- base_plot +
  geom_function(fun = function (p) dbinom(10, size = 14, prob = p), linetype = "dashed", color = "red") +
  geom_function(fun = function (p) dbinom(10, size = 15, prob = p),  color = "blue") +
  labs(title = TeX(r'($WLWWWLWLWWWWLWL$)'))

p16 <- base_plot +
  geom_function(fun = function (p) dbinom(10, size = 15, prob = p), linetype = "dashed", color = "red") +
  geom_function(fun = function (p) dbinom(11, size = 16, prob = p),  color = "blue") +
  labs(title = TeX(r'($WLWWWLWLWWWWLWLW$)'))

gridExtra::grid.arrange(p1, p2, p3, p4, 
                        p5, p6, p7, p8,
                        p9, p10, p11, p12,
                        p13, p14, p15, p16,
                        nrow = 4, ncol = 4)

```

By our 16th iteration, it seems that our most likely value for the proportion of water on the Earth's surface area is around 70%, which it turns out is a pretty accurate estimate.

## Bayes Theorem

Bayes Theorem is very simply derived and gives rise to the theoretical concept of a prior and posterior probability.  Imagine we have two events $A$ and $B$.  The we can say that the probability of both $A$ and $B$ occurring is the the probability of $A$ occurring given that $B$ occurs AND $B$ occurring. Therefore:

$$
\mathrm{Pr}(A \cap B) = \mathrm{Pr}(A \mid B)\mathrm{Pr}(B)
$$

But using the same logic we can also say:

$$
\mathrm{Pr}(A \cap B) = \mathrm{Pr}(B \mid A)\mathrm{Pr}(A)
$$

So we can conclude that:

$$
\mathrm{Pr}(A \mid B)\mathrm{Pr}(B) = \mathrm{Pr}(B \mid A)\mathrm{Pr}(A)
$$

or otherwise stated as *Bayes Theorem*:

$$
\mathrm{Pr}(A \mid B) = \frac{\mathrm{Pr}(B \mid A)\mathrm{Pr}(A)}{\mathrm{Pr}(B)}
$$
If we apply this to our globe example, it translates to:

$$
\mathrm{Pr}(p \mid W, L) = \frac{\mathrm{Pr}(W, L \mid p)\mathrm{Pr}(p)}{\mathrm{Pr}(W, L)}
$$
Note that 

* $\mathrm{Pr}(p \mid W, L)$ is the probability for the proportion of water given our sample data, so it is our posterior.  
* $\mathrm{Pr}(p)$ is the original probability for the proportion of water that we started out with, so it is our prior.
* $\mathrm{Pr}(W, L \mid p)$ is the likelihood of seeing our sample data given $p$, while $\mathrm{Pr}(W, L)$ is the overall (or average) likelihood of seeing our sample data.  So the ratio of the two represents a likelihood function which sums to 1 over all values of $p$.

Therefore we can say that:

$$
\mathrm{Posterior} = \mathrm{Likelihood} \times \mathrm{Prior} 
$$
Intuitively, our probability updates from the prior in view of the new information we get from our sample.  

## Approximation methods

Estimating a posterior distribution from a prior distribution can be complicated when you have infinitely many probabilities in a continuous distribution.  Of course, if you choose your prior to be some sort of easily manipulated formula-defined distribution, like a uniform or binomial distribution, things might be easier.  But that constrains us a great deal.  

There are three common methods for approximating a posterior: grid approximation, quadratic approximation and Markov Chain Monte Carlo (MCMC) method.  Grid approximation, though simple and easily understandable, is computationally overwhelming for all except the simplest cases, and so is unusual in modern day settings.  We quickly review all three and show an example.

### Grid approximation

We can work with any prior distribution if we approximate it using a grid (finite or limited subset) of values in the distribution.  This is known as *grid approximation*.  Using our example above, let's write a function to approximate a posterior for our 16th iteration based on a prior and a number of grid points:

```{r}
# calculate likelihood of observation
get_posterior <- function(n = 20, p_grid = seq(0, 1, length.out = n), priors = rep(1, n), ...) {
  prior <- priors
  likelihood <- dbinom(11, 16, prob = p_grid)
  # calculate posterior
  data.frame(
    p_grid = p_grid,
    posterior = (likelihood * prior)/sum(likelihood * prior)
  )
}

```

Now we plot the posterior under a uniform prior for several grid approximations -- 5, 10, 20 and 100 equally spaced points respectively.  As we might expect, the more grid points, the more precise our approximation of the posterior distribution:

```{r}
p1 <- ggplot() +
  ylim(0, 0.8) +
  geom_line(aes(x = get_posterior(5)$p_grid, y = get_posterior(5)$posterior), color = "darkblue") +
  xlab("p") +
  ylab("Posterior likelihood") +
  labs(title = "5 grid points") +
  theme_minimal()

p2 <- ggplot() +
  ylim(0, 0.4) +
  geom_line(aes(x = get_posterior(10)$p_grid, y = get_posterior(10)$posterior), color = "darkblue") +
  xlab("p") +
  ylab("Posterior likelihood") +
  labs(title = "10 grid points") +
  theme_minimal()

p3 <- ggplot() +
  ylim(0, 0.2) +
  geom_line(aes(x = get_posterior(20)$p_grid, y = get_posterior(20)$posterior), color = "darkblue") +
  xlab("p") +
  ylab("Posterior likelihood") +
  labs(title = "20 grid points") +
  theme_minimal()

p4 <- ggplot() +
  ylim(0, 0.04) +
  geom_line(aes(x = get_posterior(100)$p_grid, y = get_posterior(100)$posterior), color = "darkblue") +
  xlab("p") +
  ylab("Posterior likelihood") +
  labs(title = "100 grid points") +
  theme_minimal()
  
gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)

```

We can also experiment with different priors.  For example we can use a step prior:

```{r}
step_prior <- function(x) ifelse(x < 0.5, 0, 1)

ggplot() +
  xlim(0,1) +
  xlab("p") +
  ylab("Prior likelihood") +
  geom_function(fun = step_prior, color = "blue") +
  theme_minimal()
```

And we will find equivalent estimates of posteriors:
```{r}
p1 <- ggplot() +
  ylim(0, 0.8) +
  geom_line(aes(x = get_posterior(5, priors = step_prior(seq(0, 1, length.out = 5)))$p_grid, y = get_posterior(5, prior = step_prior(seq(0, 1, length.out = 5)))$posterior), color = "darkblue") +
  xlab("p") +
  ylab("Posterior likelihood") +
  labs(title = "5 grid points") +
  theme_minimal()

p2 <- ggplot() +
  ylim(0, 0.5) +
  geom_line(aes(x = get_posterior(10, prior = step_prior(seq(0, 1, length.out = 10)))$p_grid, y = get_posterior(10, prior = step_prior(seq(0, 1, length.out = 10)))$posterior), color = "darkblue") +
  xlab("p") +
  ylab("Posterior likelihood") +
  labs(title = "10 grid points") +
  theme_minimal()

p3 <- ggplot() +
  ylim(0, 0.25) +
  geom_line(aes(x = get_posterior(20, prior = step_prior(seq(0, 1, length.out = 20)))$p_grid, y = get_posterior(20, prior = step_prior(seq(0, 1, length.out = 20)))$posterior), color = "darkblue") +
  xlab("p") +
  ylab("Posterior likelihood") +
  labs(title = "20 grid points") +
  theme_minimal()

p4 <- ggplot() +
  ylim(0, 0.04) +
  geom_line(aes(x = get_posterior(100, prior = step_prior(seq(0, 1, length.out = 100)))$p_grid, y = get_posterior(100, prior = step_prior(seq(0, 1, length.out = 100)))$posterior), color = "darkblue") +
  xlab("p") +
  ylab("Posterior likelihood") +
  labs(title = "100 grid points") +
  theme_minimal()
  
gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)

```

And similarly with a peaked prior:

```{r}
peaked_prior <- function(x) exp(-5 * abs(x - 0.5))

ggplot() +
  xlim(0,1) +
  xlab("p") +
  ylab("Prior likelihood") +
  geom_function(fun = peaked_prior, color = "blue") +
  theme_minimal()
```

We get the following:

```{r}
p1 <- ggplot() +
  ylim(0, 0.8) +
  geom_line(aes(x = get_posterior(5, prior = peaked_prior(seq(0, 1, length.out = 5)))$p_grid, y = get_posterior(5, prior = peaked_prior(seq(0, 1, length.out = 5)))$posterior), color = "darkblue") +
  xlab("p") +
  ylab("Posterior likelihood") +
  labs(title = "5 grid points") +
  theme_minimal()

p2 <- ggplot() +
  ylim(0, 0.4) +
  geom_line(aes(x = get_posterior(10, prior = peaked_prior(seq(0, 1, length.out = 10)))$p_grid, y = get_posterior(10, prior = peaked_prior(seq(0, 1, length.out = 10)))$posterior), color = "darkblue") +
  xlab("p") +
  ylab("Posterior likelihood") +
  labs(title = "10 grid points") +
  theme_minimal()

p3 <- ggplot() +
  ylim(0, 0.2) +
  geom_line(aes(x = get_posterior(20, prior = peaked_prior(seq(0, 1, length.out = 20)))$p_grid, y = get_posterior(20, prior = peaked_prior(seq(0, 1, length.out = 20)))$posterior), color = "darkblue") +
  xlab("p") +
  ylab("Posterior likelihood") +
  labs(title = "20 grid points") +
  theme_minimal()

p4 <- ggplot() +
  ylim(0, 0.04) +
  geom_line(aes(x = get_posterior(100, prior = peaked_prior(seq(0, 1, length.out = 100)))$p_grid, y = get_posterior(100, prior = peaked_prior(seq(0, 1, length.out = 100)))$posterior), color = "darkblue") +
  xlab("p") +
  ylab("Posterior likelihood") +
  labs(title = "100 grid points") +
  theme_minimal()
  
gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)

```

Often we are in a position to make an intelligent guess at a prior.  For example, maybe we are sure that more than half but less than three quarters of the Earth's surface area is covered in water.  So we can define a prior that is uniform between 0.5 and 0.75 and zero elsewhere.

```{r}
intelligent_guess_prior <- function(x) ifelse(x < 0.5, 0, ifelse(x > 0.75, 0, 1))

ggplot() +
  xlim(0,1) +
  xlab("p") +
  ylab("Prior likelihood") +
  geom_function(fun = intelligent_guess_prior, color = "blue") +
  theme_minimal()

```

This would then give rise to the following posterior (this time let's just look at the most precise one):

```{r}
ggplot() +
  ylim(0, 0.06) +
  geom_line(aes(x = get_posterior(100, prior = intelligent_guess_prior(seq(0, 1, length.out = 100)))$p_grid, y = get_posterior(100, prior = intelligent_guess_prior(seq(0, 1, length.out = 100)))$posterior), color = "darkblue") +
  xlab("p") +
  ylab("Posterior likelihood") +
  theme_minimal()
```

### Quadratic (Laplace) approximation

We can expect in general that the area near the peak of our posterior density curve will have a shape resembling a normal distribution.  You'll have noticed this in all of our charts above.  Since we know that normal distributions can be characterized by their center (mean) and spread (standard deviation/variance), this allows us a route to approxmation of our posterior.  

We know that the logarithm of the normal distribution takes a quadratic form (a parabola), so whenever we expect a log-posterior to be parabolic, we can approximate it with a quadratic function.  This is called *quadratic approximation*.

Here's an example using the `rethinking` package of how to use a quadratic approximation to estimate our posterior distribution for our global water example assuming a uniform prior.

```{r, message = FALSE, warning = FALSE}
library(rethinking)

# perform a quadratic approximation of posterior based on a likelihood and a prior
globalwater.qa <- quap(
  alist(
    # formulas for likelihood and prior 
    W ~ dbinom(W + L, p),
    p ~ dunif(0, 1)
  ),
  # actual data
  data = list(W = 11, L = 5)
)

# display summary of the approximation
(precis_water <- precis(globalwater.qa))
```

We can see the approximation suggests a mean of `r round(precis_water$mean, 2)` and a standard deviation of `r round(precis_water$sd, 2)`.  Let's plot it (blue line) alongside a precise curve for our posterior (red dashed line).

```{r}
ggplot() +
  xlim(0, 1) +
  xlab(TeX(r'($p$)')) +
  ylab("Density") + 
  theme_minimal() +
  geom_function(fun = function (p) dbeta(p, 12, 6),  color = "red", linetype = "dashed") +
  geom_function(fun = function (p) dnorm(p, mean = precis_water$mean, sd = precis_water$sd),  color = "blue")


```

With larger amounts of data - but the same sample proportions - we can achieve even more precise approximations of our posterior.  For example, Let's assume we had a sample of 110 water observations and 50 land obervations.

```{r}
globalwater.qa110 <- quap(
  alist(
    W ~ dbinom(W + L, p),
    p ~ dunif(0, 1)
  ),
  data = list(W = 110, L = 50)
)

precis_water110 <- precis(globalwater.qa110)

ggplot() +
  xlim(0, 1) +
  xlab(TeX(r'($p$)')) +
  ylab("Density") + 
  theme_minimal() +
  geom_function(fun = function (p) dbeta(p, 111, 51),  color = "red", linetype = "dashed") +
  geom_function(fun = function (p) dnorm(p, mean = precis_water110$mean, sd = precis_water110$sd),  color = "blue")
```

This method is very similar to Maximum Likelihood Estimation - a common procedure in fitting standard regression models.  It is not perfect and -- depending on the sample -- it can poorly estimate the posterior even with a large amount of data.  It can also fail if it cannot compute the Hessian (which is required to estimate the standard deviation of the normal distribution).   

### Markov Chain Monte Carlo

This approximation method is not intuitive but often works and can handle high complexity posteriors.  It estimates the posterior by simply calculating the probabilities over a set of random datapoints, and the using the frequency distribution of the results. Here's an example of how we can use this approach to approximate our global water posterior, using 100,000 samples:

```{r}
# use 100,000 samples
n_samples <- 100000

# start a posterior probability vector
p <- c()

W <- 11
L <- 5

p[1] <- 0.5

for (i in 2:n_samples) {
  # random deviation from previous
  p_new <- rnorm(1, p[i-1], 0.1)
  # take care of outliers
  if (p_new < 0) p_new <- abs(p_new)
  if (p_new > 1) p_new <- 2 - p_new
  # construct likelihood
  q0 <- dbinom(W, W + L, p[i - 1])
  q1 <- dbinom(W, W+L, p_new)
  p[i] <- ifelse(runif(1) < q1/q0, p_new, p[i - 1])
}
```

Now we can plot our estimated density function for our posterior (blue) against the real posterior distribution (red dashed):

```{r}
ggplot() +
  xlim(0, 1) +
  xlab(TeX(r'($p$)')) +
  ylab("Density") + 
  theme_minimal() +
  geom_density(aes(x = p), color = "blue") +
  geom_function(fun = function (p) dbeta(p, 11, 5),  color = "red", linetype = "dashed")
  

```
